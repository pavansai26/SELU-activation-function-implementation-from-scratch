{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaled Exponentially Linear Unit.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNmZNkYmbY2gjsTTbyBye+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/SELU-activation-function-implementation-from-scratch/blob/master/Scaled_Exponentially_Linear_Unit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADLk_M82idPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CatE5d8_QCiC",
        "colab_type": "text"
      },
      "source": [
        "## why it is called selu?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Snd00PQQf6",
        "colab_type": "text"
      },
      "source": [
        "## what are the types of normalization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56q8wFcbQfmo",
        "colab_type": "text"
      },
      "source": [
        "# why selu is so much special?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jQxF8n7Qq9Q",
        "colab_type": "text"
      },
      "source": [
        "## what is banach fixed point theorm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhcbPwMMSCyy",
        "colab_type": "text"
      },
      "source": [
        "# advantages of selu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLnYsjCAiqE5",
        "colab_type": "text"
      },
      "source": [
        "# **Scaled Exponentially Linear Unit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGFUKApBi0yL",
        "colab_type": "text"
      },
      "source": [
        "formula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapeYsSoi3Cl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$ \\text{SELU}\\left(x\\right)) = \\lambda_{selu}x \\text{ if } x \\geq 0 $$ $$ \\text{SELU}\\left(x\\right)) = \\lambda_{selu}\\alpha_{selu}xe^{x}-1 \\text{ if } x < 0 $$\n",
        "\n",
        "where the two parameters $\\lambda_{selu} > 0$ and $\\alpha_{selu} > 0$ remain to be specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d5PHA5KiohZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def selu(x, lambdaa=1.07862, alphaa=2.90427):\n",
        "\n",
        "  if x>=0:\n",
        "    return lambdaa*x\n",
        "  \n",
        "  else:\n",
        "    return lambdaa* alphaa*x*np.exp(x)-1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES-w_04QjVKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "#Scaled Exponentially Linear Unit\n",
        "def selu(x, lambdaa=1.07862, alphaa=2.90427):\n",
        "\n",
        "  if x>=0:\n",
        "    return lambdaa*x\n",
        "  \n",
        "  else:\n",
        "    return lambdaa* alphaa*x*np.exp(x)-1\n",
        "\n",
        "# define a series of inputs\n",
        "series_in = [x for x in range(-10, 11)]\n",
        "# calculate outputs for our inputs\n",
        "series_out = [selu(x, lambdaa=1.07862, alphaa=2.90427) for x in series_in]\n",
        "# line plot of raw inputs to rectified outputs\n",
        "pyplot.plot(series_in, series_out)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tObob9VWjoX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "#Scaled Exponentially Linear Unit\n",
        "def selu(x, lambdaa=1.07862, alphaa=2.90427):\n",
        "\n",
        "  if x>=0:\n",
        "    return lambdaa*x\n",
        "  \n",
        "  else:\n",
        "    return lambdaa* alphaa*x*np.exp(x)-1\n",
        "\n",
        "# define a series of inputs\n",
        "series_in = [x for x in range(-10, 11)]\n",
        "# calculate outputs for our inputs\n",
        "series_out = [selu(x, lambdaa=5, alphaa=7) for x in series_in]\n",
        "# line plot of raw inputs to rectified outputs\n",
        "pyplot.plot(series_in, series_out)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2l_gXtvkaZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}